{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import gc\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_coord(lat_min,lat_max,lon_min,lon_max,step,precision):\n",
    "    coords = list(itertools.product(\n",
    "        [format(x,f'.{precision}f') for x in np.arange(lat_min,lat_max+step,step)],\n",
    "        [format(x,f'.{precision}f') for x in np.arange(lon_min,lon_max+step,step)]))    \n",
    "\n",
    "    string_coords = [f'({item[0]} {item[1]})' for item in coords]\n",
    "    return string_coords\n",
    "\n",
    "def concat_tables_satelites(source,data,verbose=False):\n",
    "    final_df = None\n",
    "    for i_coord,coordinate in enumerate(coordinates[source]):\n",
    "        df = spark.read.csv(f'{path_dict[source][data]}{string_coordinates[source][i_coord]}.csv',header=True,inferSchema=True)\n",
    "        if verbose:\n",
    "            print(f'{coordinate} ({i_coord+1}/{len(coordinates[source])})',end='\\r')\n",
    "        df = df \\\n",
    "            .withColumn('lat',F.lit(coordinate['lat'])) \\\n",
    "            .withColumn('lon',F.lit(coordinate['lon']))\n",
    "        if final_df is None:\n",
    "            final_df = df\n",
    "        elif not final_df is None:\n",
    "            final_df = final_df.union(df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def polars_concat_tables_satelites(source, data, verbose=False):\n",
    "    final_df = None\n",
    "    for i_coord, coordinate in enumerate(coordinates[source]):\n",
    "        # Construir o caminho para o arquivo CSV\n",
    "        file_path = f'{path_dict[source][data]}{string_coordinates[source][i_coord]}.csv'\n",
    "        \n",
    "        # Ler o arquivo CSV usando Polars\n",
    "        df = pl.read_csv(file_path)\n",
    "        \n",
    "        # Adicionar colunas de latitude e longitude\n",
    "        df = df.with_columns([\n",
    "            pl.lit(coordinate['lat']).alias('lat'),\n",
    "            pl.lit(coordinate['lon']).alias('lon')\n",
    "        ])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'{coordinate} ({i_coord+1}/{len(coordinates[source])})', end='\\r')\n",
    "        \n",
    "        # Concatenar DataFrames\n",
    "        if final_df is None:\n",
    "            final_df = df\n",
    "        else:\n",
    "            final_df = final_df.vstack(df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def satelites_join_medidas(df_dict):\n",
    "    joined_df = None\n",
    "    for df in df_dict.values():\n",
    "        if joined_df is None:\n",
    "            joined_df = df\n",
    "        else:\n",
    "            joined_df = joined_df.join(df,['data','lat','lon'],'outer')\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Mateus\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128MB\") \\\n",
    "    .config(\"spark.sql.files.openCostInBytes\", \"4MB\")  \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\")  \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\")  \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Satélites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Obtendo metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_path = 'C:/Users/Mateus Santos Rochas/Desktop/Estudos/07. Doutorado - Matemática aplicada/Pesquisa/Dados R2'\n",
    "bronze_path = os.path.join(initial_path,'1. bronze')\n",
    "\n",
    "path_dict = {\n",
    "    'AgCFSR':{\n",
    "        'prate':os.path.join(initial_path,r'0. bruto\\AgCFSR_prate\\AgCFSR\\agcfsr_prate_')\n",
    "        ,'rhstmax':os.path.join(initial_path,r'0. bruto\\AgCFSR_rhstmax\\AgCFSR\\agcfsr_rhstmax_')\n",
    "        ,'srad':os.path.join(initial_path,r'0. bruto\\AgCFSR_srad\\AgCFSR\\agcfsr_srad_')\n",
    "        ,'tavg':os.path.join(initial_path,r'0. bruto\\AgCFSR_tavg\\AgCFSR\\agcfsr_tavg_')\n",
    "        ,'tmax':os.path.join(initial_path,r'0. bruto\\AgCFSR_tmax\\AgCFSR\\agcfsr_tmax_')\n",
    "        ,'tmin':os.path.join(initial_path,r'0. bruto\\AgCFSR_tmin\\AgCFSR\\agcfsr_tmin_')\n",
    "        ,'wndspd':os.path.join(initial_path,r'0. bruto\\AgCFSR_wndspd\\AgCFSR\\agcfsr_wndspd_')\n",
    "    }\n",
    "    ,'AgMERRA':{\n",
    "        'prate':os.path.join(initial_path,r'0. bruto\\AgMERRA_prate\\AgMERRA\\agmerra_prate_')\n",
    "        ,'rhstmax':os.path.join(initial_path,r'0. bruto\\AgMERRA_rhstmax\\AgMERRA\\agmerra_rhstmax_')\n",
    "        ,'srad':os.path.join(initial_path,r'0. bruto\\AgMERRA_srad\\AgMERRA\\agmerra_srad_')\n",
    "        ,'tavg':os.path.join(initial_path,r'0. bruto\\AgMERRA_tavg\\AgMERRA\\agmerra_tavg_')\n",
    "        ,'tmax':os.path.join(initial_path,r'0. bruto\\AgMERRA_tmax\\AgMERRA\\agmerra_tmax_')\n",
    "        ,'tmin':os.path.join(initial_path,r'0. bruto\\AgMERRA_tmin\\AgMERRA\\agmerra_tmin_')\n",
    "        ,'wndspd':os.path.join(initial_path,r'0. bruto\\AgMERRA_wndspd\\AgMERRA\\agmerra_wndspd_')\n",
    "    }\n",
    "    ,'CHIRPS':{\n",
    "        'precip':os.path.join(initial_path,r'0. bruto\\CHIRPS_precipitation\\CHIRPS\\chirps_precipitation_')\n",
    "    }\n",
    "    ,'CPC':{\n",
    "        'precip':os.path.join(initial_path,r'0. bruto\\CPC_precip\\CPC\\cpc_precip_')\n",
    "        ,'tmax':os.path.join(initial_path,r'0. bruto\\CPC_tmax\\CPC\\cpc_tmax_')\n",
    "        ,'tmin':os.path.join(initial_path,r'0. bruto\\CPC_tmin\\CPC\\cpc_tmin_')\n",
    "    }\n",
    "    ,'GL':{\n",
    "        'irradiancia':os.path.join(initial_path,r'0. bruto\\GL_irradiancia\\GL\\gl_irradiancia_')\n",
    "    }\n",
    "    ,'GPM Final Run':{\n",
    "        'precipitation':os.path.join(initial_path,r'0. bruto\\GPM Final Run_precipitation\\GPM Final Run\\gpm-final-run_precipitation_')\n",
    "    }\n",
    "    ,'GPM Late Run':{\n",
    "        'precipitation':os.path.join(initial_path,r'0. bruto\\GPM Late Run_precipitation\\GPM Late Run\\gpm-late-run_precipitation_')\n",
    "    }\n",
    "    ,'PERSIANN-CDR':{\n",
    "        'precipitation':os.path.join(initial_path,r'0. bruto\\PERSIANN-CDR_precipitation\\PERSIANN-CDR\\persiann_precipitation_')\n",
    "    }\n",
    "    ,'POWER':{\n",
    "        'allsky_sfc_sw_dwn':os.path.join(initial_path,r'0. bruto\\POWER_allsky_sfc_sw_dwn\\POWER\\power_allsky_sfc_sw_dwn_')\n",
    "        ,'prectotcorr':os.path.join(initial_path,r'0. bruto\\POWER_prectotcorr\\POWER\\power_prectotcorr_')\n",
    "        ,'ps':os.path.join(initial_path,r'0. bruto\\POWER_ps\\POWER\\power_ps_')\n",
    "        ,'rh2m':os.path.join(initial_path,r'0. bruto\\POWER_rh2m\\POWER\\power_rh2m_')\n",
    "        ,'t2m':os.path.join(initial_path,r'0. bruto\\POWER_t2m\\POWER\\power_t2m_')\n",
    "        ,'t2m_max':os.path.join(initial_path,r'0. bruto\\POWER_t2m_max\\POWER\\power_t2m_max_')\n",
    "        ,'t2m_min':os.path.join(initial_path,r'0. bruto\\POWER_t2m_min\\POWER\\power_t2m_min_')\n",
    "        ,'t2mdew':os.path.join(initial_path,r'0. bruto\\POWER_t2mdew\\POWER\\power_t2mdew_')\n",
    "        ,'wd2m':os.path.join(initial_path,r'0. bruto\\POWER_wd2m\\POWER\\power_wd2m_')\n",
    "        ,'wd10m':os.path.join(initial_path,r'0. bruto\\POWER_wd10m\\POWER\\power_wd10m_')\n",
    "        ,'ws2m':os.path.join(initial_path,r'0. bruto\\POWER_ws2m\\POWER\\power_ws2m_')\n",
    "        ,'ws2m_max':os.path.join(initial_path,r'0. bruto\\POWER_ws2m_max\\POWER\\power_ws2m_max_')\n",
    "        ,'ws10m':os.path.join(initial_path,r'0. bruto\\POWER_ws10m\\POWER\\power_ws10m_')\n",
    "        ,'ws10m_max':os.path.join(initial_path,r'0. bruto\\POWER_ws10m_max\\POWER\\power_ws10m_max_')\n",
    "    }\n",
    "    ,'TRMM':{\n",
    "        'precipitation':os.path.join(initial_path,r'0. bruto\\TRMM_precipitation\\TRMM\\trmm_precipitation_')\n",
    "    }\n",
    "}\n",
    "\n",
    "string_coordinates = {\n",
    "    'AgCFSR':build_coord(-24,-17,-53,-38,0.25,2)\n",
    "    ,'AgMERRA':build_coord(-24,-17,-53,-38,0.25,2)\n",
    "    ,'CHIRPS':build_coord(-24,-17,-53,-38,0.05,2)\n",
    "    ,'CPC':build_coord(-24,-17,-53,-38,0.05,2)\n",
    "    ,'GL':build_coord(-24,-17.1,-53,-38,0.1,1)\n",
    "    ,'GPM Final Run':build_coord(-24,-17.1,-53,-38,0.1,1)\n",
    "    ,'GPM Late Run':build_coord(-24,-17.1,-53,-38,0.1,1)\n",
    "    ,'PERSIANN-CDR':build_coord(-24,-17,-53,-38,0.25,2)\n",
    "    ,'POWER':build_coord(-24,-17,-53,-38,0.5,1)\n",
    "    ,'TRMM':build_coord(-24,-17,-53,-38,0.25,2)\n",
    "}\n",
    "\n",
    "sources = list(path_dict.keys())\n",
    "datas = {source:list(path_dict[source].keys()) for source in sources}\n",
    "\n",
    "coordinates = {}\n",
    "for source in sources:\n",
    "    coordinates[source] = [\n",
    "        {\n",
    "        'lat':float(string_coordinate.split(' ')[0][1:]),\n",
    "        'lon':float(string_coordinate.split(' ')[1][:-1])\n",
    "        } for string_coordinate in string_coordinates[source]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Obtendo as tabelas e concatenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: AgCFSR (1/10)\n",
      "    Data: prate (1/7)\n",
      "    Data: rhstmax (2/7)38.0} (1769/1769)))\n",
      "    Data: srad (3/7): -38.0} (1769/1769)))\n",
      "    Data: tavg (4/7): -38.0} (1769/1769)))\n",
      "    Data: tmax (5/7): -38.0} (1769/1769)))\n",
      "    Data: tmin (6/7): -38.0} (1769/1769)))\n",
      "    Data: wndspd (7/7)-38.0} (1769/1769)))\n",
      "Source: AgMERRA (2/10)-38.0} (1769/1769)))\n",
      "    Data: prate (1/7)\n",
      "    Data: rhstmax (2/7)38.0} (1769/1769)))\n",
      "    Data: srad (3/7): -38.0} (1769/1769)))\n",
      "    Data: tavg (4/7): -38.0} (1769/1769)))\n",
      "    Data: tmax (5/7): -38.0} (1769/1769)))\n",
      "    Data: tmin (6/7): -38.0} (1769/1769)))\n",
      "    Data: wndspd (7/7)-38.0} (1769/1769)))\n",
      "{'lat': -17.0, 'lon': -38.0} (1769/1769)))\r"
     ]
    }
   ],
   "source": [
    "sources_batch_1 = ['AgCFSR','AgMERRA']\n",
    "for i_s,source in enumerate(sources_batch_1):\n",
    "    print(f'Source: {source} ({i_s+1}/{len(sources_batch_1)})')\n",
    "    for i_d,data in enumerate(datas[source]):\n",
    "        print(f'    Data: {data} ({i_d+1}/{len(datas[source])})')\n",
    "        df = polars_concat_tables_satelites(source,data,verbose=True)\n",
    "        df.write_csv(os.path.join(initial_path, '1. bronze', f'{source}_{data}.csv'))\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "        ### Tentar separar em blocos de 500/1000 para juntar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: CHIRPS (1/10)\n",
      "    Data: precip (1/1)\n",
      "{'lat': -17.0, 'lon': -38.0} (42441/42441)))\r"
     ]
    }
   ],
   "source": [
    "sources_batch_2 = ['CHIRPS']\n",
    "for i_s,source in enumerate(sources_batch_2):\n",
    "    print(f'Source: {source} ({i_s+1}/{len(sources_batch_2)})')\n",
    "    for i_d,data in enumerate(datas[source]):\n",
    "        print(f'    Data: {data} ({i_d+1}/{len(datas[source])})')\n",
    "        df = polars_concat_tables_satelites(source,data,verbose=True)\n",
    "        df.write_csv(os.path.join(initial_path, '1. bronze', f'{source}_{data}.csv'))\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: GPM Final Run (1/10)\n",
      "    Data: precipitation (1/1)\n",
      "Source: GPM Late Run (2/10)} (10570/10570)\n",
      "    Data: precipitation (1/1)\n",
      "{'lat': -17.1, 'lon': -38.0} (10570/10570)\r"
     ]
    }
   ],
   "source": [
    "# sources_batch_3 = ['CPC','GL','GPM Final Run','GPM Late Run']\n",
    "sources_batch_3 = ['GPM Final Run','GPM Late Run']\n",
    "for i_s,source in enumerate(sources_batch_3):\n",
    "    print(f'Source: {source} ({i_s+1}/{len(sources_batch_3)})')\n",
    "    for i_d,data in enumerate(datas[source]):\n",
    "        print(f'    Data: {data} ({i_d+1}/{len(datas[source])})')\n",
    "        df = polars_concat_tables_satelites(source,data,verbose=True)\n",
    "        df.write_csv(os.path.join(initial_path, '1. bronze', f'{source}_{data}.csv'))\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: PERSIANN-CDR (1/10)\n",
      "    Data: precipitation (1/1)\n",
      "Source: POWER (2/10): -38.0} (1769/1769)))\n",
      "    Data: allsky_sfc_sw_dwn (1/14)\n",
      "    Data: prectotcorr (2/14) (465/465)\n",
      "    Data: ps (3/14)': -38.0} (465/465)\n",
      "    Data: rh2m (4/14) -38.0} (465/465)\n",
      "    Data: t2m (5/14): -38.0} (465/465)\n",
      "    Data: t2m_max (6/14)8.0} (465/465)\n",
      "    Data: t2m_min (7/14)8.0} (465/465)\n",
      "    Data: t2mdew (8/14)38.0} (465/465)\n",
      "    Data: wd2m (9/14) -38.0} (465/465)\n",
      "    Data: wd10m (10/14)38.0} (465/465)\n",
      "    Data: ws2m (11/14)-38.0} (465/465)\n",
      "    Data: ws2m_max (12/14)0} (465/465)\n",
      "    Data: ws10m (13/14)38.0} (465/465)\n",
      "    Data: ws10m_max (14/14)} (465/465)\n",
      "Source: TRMM (3/10)': -38.0} (465/465)\n",
      "    Data: precipitation (1/1)\n",
      "{'lat': -17.0, 'lon': -38.0} (1769/1769)))\r"
     ]
    }
   ],
   "source": [
    "sources_batch_4 = ['PERSIANN-CDR','POWER','TRMM']\n",
    "for i_s,source in enumerate(sources_batch_4):\n",
    "    print(f'Source: {source} ({i_s+1}/{len(sources_batch_4)})')\n",
    "    for i_d,data in enumerate(datas[source]):\n",
    "        print(f'    Data: {data} ({i_d+1}/{len(datas[source])})')\n",
    "        df = polars_concat_tables_satelites(source,data,verbose=True)\n",
    "        df.write_csv(os.path.join(initial_path, '1. bronze', f'{source}_{data}.csv'))\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Unindo as tabelas de cada fonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "medidas = {\n",
    "    'AgCFSR':['prate','rhstmax','srad','tavg','tmax','tmin','wndspd'],\n",
    "    'AgMERRA':['prate','rhstmax','srad','tavg','tmax','tmin','wndspd'],\n",
    "    'CHIRPS':['precipitation'],\n",
    "    'CPC':['precipitation','tmax','tmin'],\n",
    "    'GL':['irradiancia'],\n",
    "    'GPM Final Run':['precipitation'],\n",
    "    'GPM Late Run':['precipitation'],\n",
    "    'PERSIANN-CDR':['precipitation'],\n",
    "    'POWER':['allsky_sfc_sw_dwn','prectotcorr','ps','rh2m','t2m_max','t2m_min','t2m','t2mdew','wd2m','wd10m','ws2m_max','ws2m','ws10m_max','ws10m'],\n",
    "    'TRMM':['precipitation']\n",
    "}\n",
    "\n",
    "satelites_dict = {satelite:{medida:spark.read.csv(os.path.join(bronze_path,f'{satelite}_{medida}.csv'),\n",
    "                              header=True,inferSchema=True) \\\n",
    "                                .withColumnRenamed('valor',f'vl_{medida}') for medida in medidas[satelite]} for satelite in medidas.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o CSV resultado dos joins\n",
    "satelites = {satelite:satelites_join_medidas(satelite_dict) for satelite,satelite_dict in satelites_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for satelite,df in satelites.items():\n",
    "    df.write.option(\"header\",True) \\\n",
    "    .option(\"delimiter\",\",\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(os.path.join(initial_path, '1. bronze', f'full_{satelite}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Estações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estacoes_path = os.path.join(initial_path,'0. bruto','estacoes')\n",
    "estacoes_bronze_path = os.path.join(initial_path,'1. bronze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. cemaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3903568391.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cemaden = cemaden.replace(r'^\\s+$', np.nan, regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Importando a tabela\n",
    "cemaden = pd.read_csv(os.path.join(estacoes_path,'cemaden.csv'),sep='|').iloc[1:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "cemaden.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Substituir valores que contenham qualquer número de espaços por np.nan em todas as colunas\n",
    "cemaden = cemaden.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "# Escrevendo na bronze\n",
    "cemaden.to_csv(os.path.join(estacoes_bronze_path,'cemaden.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. cemig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\967585047.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  cemig = pd.read_csv(os.path.join(estacoes_path,'CEMIG.csv'),sep=';').iloc[:-1]\n"
     ]
    }
   ],
   "source": [
    "# Importando a tabela\n",
    "cemig = pd.read_csv(os.path.join(estacoes_path,'CEMIG.csv'),sep=';').iloc[:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "cemig.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Escrevendo na bronze\n",
    "cemig.to_csv(os.path.join(estacoes_bronze_path,'cemig.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. iac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3991268309.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  iac = pd.read_csv(os.path.join(estacoes_path,'IAC.csv'),sep='|').iloc[1:-1]\n",
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3991268309.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  iac = iac.replace(r'^\\s+$', np.nan, regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Importando a tabela\n",
    "iac = pd.read_csv(os.path.join(estacoes_path,'IAC.csv'),sep='|').iloc[1:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "iac.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Substituir valores que contenham qualquer número de espaços por np.nan em todas as colunas\n",
    "iac = iac.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "# Escrevendo na bronze\n",
    "iac.to_csv(os.path.join(estacoes_bronze_path,'iac.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. inmet_agri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3440553913.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  inmet_agri = pd.read_csv(os.path.join(estacoes_path,'INMET_AGRI.csv'),sep='|').iloc[1:-1]\n",
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3440553913.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  inmet_agri = inmet_agri.replace(r'^\\s+$', np.nan, regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Importando a tabela\n",
    "inmet_agri = pd.read_csv(os.path.join(estacoes_path,'INMET_AGRI.csv'),sep='|').iloc[1:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "inmet_agri.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Substituir valores que contenham qualquer número de espaços por np.nan em todas as colunas\n",
    "inmet_agri = inmet_agri.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "# Escrevendo na bronze\n",
    "inmet_agri.to_csv(os.path.join(estacoes_bronze_path,'inmet_agri.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. inmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\2677540370.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  inmet = pd.read_csv(os.path.join(estacoes_path,'INMET.csv'),sep=';').iloc[:-1]\n"
     ]
    }
   ],
   "source": [
    "# Importando a tabela\n",
    "inmet = pd.read_csv(os.path.join(estacoes_path,'INMET.csv'),sep=';').iloc[:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "inmet.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Escrevendo na bronze\n",
    "inmet.to_csv(os.path.join(estacoes_bronze_path,'inmet.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. unesp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3492171266.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  unesp = pd.read_csv(os.path.join(estacoes_path,'unesp.csv'),sep='|').iloc[1:-1]\n",
      "C:\\Users\\Mateus Santos Rochas\\AppData\\Local\\Temp\\ipykernel_26716\\3492171266.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unesp = unesp.replace(r'^\\s+$', np.nan, regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Importando a tabela\n",
    "unesp = pd.read_csv(os.path.join(estacoes_path,'unesp.csv'),sep='|').iloc[1:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "unesp.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Substituir valores que contenham qualquer número de espaços por np.nan em todas as colunas\n",
    "unesp = unesp.replace(r'^\\s+$', np.nan, regex=True)\n",
    "\n",
    "# Escrevendo na bronze\n",
    "unesp.to_csv(os.path.join(estacoes_bronze_path,'unesp.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. z_ana_hidro_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a tabela\n",
    "ana = pd.read_csv(os.path.join(estacoes_path,'z_ana_hidro_old.csv'),sep=';').iloc[0:-1]\n",
    "\n",
    "# Renomeando as colunas\n",
    "ana.columns = ['id_estacao','lat','lon','data','tmin','tmed','tmax','prec','urmin','urmed','urmax']\n",
    "\n",
    "# Escrevendo na bronze\n",
    "ana.to_csv(os.path.join(estacoes_bronze_path,'ana.csv'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
